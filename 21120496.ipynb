{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaYuOSbeJWkd00Ire3V36l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChuLinh02/DeepLearning/blob/main/21120496.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-channel LSTM-CNN Model for Text Classification**\n",
        "\n",
        "**Name**: Chu Hải Linh\n",
        "\n",
        "**MSSV**: 21120496\n",
        "\n",
        "**Class**: Deep Learning - CQ2021/24"
      ],
      "metadata": {
        "id": "pdDj6FmyewA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import necessary Libraries**\n",
        "\n",
        "I will use **TensorFlow** for model creation and preprocessing, and **scikit-learn** for evaluation."
      ],
      "metadata": {
        "id": "iLkuOJTne6dn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YxRnD6FGNusy"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Load Dataset**\n",
        "\n",
        "I use the **IMDb dataset**, which contains 25,000 movie reviews labeled as positive or negative.\n",
        "\n",
        "This dataset is pre-tokenized and can be loaded directly using TensorFlow."
      ],
      "metadata": {
        "id": "cGJrm9pTOnKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "print(\"Loading data...\")\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Print dataset info\n",
        "print(\"Number of training samples:\", len(x_train))\n",
        "print(\"Number of testing samples:\", len(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZt4D05iOi7f",
        "outputId": "1c010624-5806-42ff-dc4a-92406f53f9a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Number of training samples: 25000\n",
            "Number of testing samples: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Preprocess the Data**\n",
        "\n",
        "To prepare the dataset for the model, I:\n",
        "\n",
        "* Pad sequences to ensure uniform length.\n",
        "\n",
        "* Define a maximum sequence length (`maxlen`)\n"
      ],
      "metadata": {
        "id": "L7ylKP11O6H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing data...\")\n",
        "\n",
        "# Define a maximum sequence length\n",
        "maxlen = 200\n",
        "\n",
        "# Pad sequences\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Split the training set into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the sizes of the new splits\n",
        "print(f\"Training set size: {len(x_train)}\")\n",
        "print(f\"Validation set size: {len(x_val)}\")\n",
        "print(f\"Testing set size: {len(x_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkgADALQO-D-",
        "outputId": "dd03cdb3-d16d-4426-b077-94e1fe4778d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data...\n",
            "Training set size: 20000\n",
            "Validation set size: 5000\n",
            "Testing set size: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Build the Multi-channel LSTM-CNN Model**\n",
        "\n",
        "I create two branches:\n",
        "\n",
        "* **LSTM Branch**: for capturing sequential dependencies.\n",
        "\n",
        "* **CNN Branch**: for capturing local patterns (n-grams).\n",
        "\n",
        "These are combined using a `Concatenate` layer, followed by full connected layers for classification."
      ],
      "metadata": {
        "id": "zlWZE8E8PDNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building model...\")\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(maxlen,))\n",
        "\n",
        "# Embedding layer\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=128, input_length=maxlen)(input_layer)\n",
        "\n",
        "# LSTM branch\n",
        "lstm_output= LSTM(128)(embedding_layer)\n",
        "\n",
        "# CNN branch\n",
        "conv_output = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding_layer)\n",
        "conv_output = MaxPooling1D(pool_size=2)(conv_output)\n",
        "conv_output = Flatten()(conv_output)\n",
        "\n",
        "# Concatenate LSTM and CNN features\n",
        "merged = Concatenate()([lstm_output, conv_output])\n",
        "dropout = Dropout(0.5)(merged)\n",
        "dense = Dense(64, activation='relu')(dropout)\n",
        "output_layer = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "uMK03Wb4PGg1",
        "outputId": "f3c2c00b-d8e1-4fc1-c5f5-1937c3045b5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12672</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],            │\n",
              "│                           │                        │                │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">819,264</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m1,280,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m49,280\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m131,584\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12672\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],            │\n",
              "│                           │                        │                │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m819,264\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,280,193</span> (8.70 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,280,193\u001b[0m (8.70 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,280,193</span> (8.70 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,280,193\u001b[0m (8.70 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Compile and train the model**\n",
        "\n",
        "I use the Adam optimizer and binary crossentropy loss function for binary classification.\n",
        "\n",
        "The model is trained for 5 epochs."
      ],
      "metadata": {
        "id": "DhogWIXDPSr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Training model...\")\n",
        "\n",
        "# Define callbacks for EarlyStopping and ModelCheckpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Training model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping, model_checkpoint]# Prevent overfitting\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHnzWAO6QpA2",
        "outputId": "0dba3a9b-5510-4d04-d6b1-b596c64e2509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Epoch 1/5\n",
            "\u001b[1m 13/157\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 1s/step - accuracy: 0.4990 - loss: 0.6950   "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Evaluate the model**\n",
        "\n",
        "I use the test set to evaluate the model and generate a classification report"
      ],
      "metadata": {
        "id": "DWrssMhaRFTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test Accuracy: {test_acc:2.f}%\")\n",
        "print(f\"Test Loss: {test_loss:2.f}%\")"
      ],
      "metadata": {
        "id": "HtQSQPOgRKlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Display prediction results**"
      ],
      "metadata": {
        "id": "3pZ1z1ffjqXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some examples from the test dataset\n",
        "index_to_word = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in index_to_word.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '') for i in text if reverse_word_index.get(i - 3)])\n",
        "\n",
        "# Select 5 random samples to display\n",
        "import numpy as np\n",
        "sample_indices = np.random.choice(len(x_test), size=5, replace=False)\n",
        "\n",
        "for i in sample_indices:\n",
        "    review = decode_review(x_test[i])\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Actual Label: {'Positive' if y_test[i] == 1 else 'Negative'}\")\n",
        "    print(f\"Predicted Label: {'Positive' if y_pred[i] == 1 else 'Negative'}\")\n",
        "    print(\"-\"*80)\n"
      ],
      "metadata": {
        "id": "YF0Tnn3yjp4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot loss and accuracy**"
      ],
      "metadata": {
        "id": "i-nPGecKk7iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I1BbxJWQkTbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing with custom data**\n",
        "\n",
        "In this section, we test the model with custom reviews that are not part of the dataset. The goal is to evaluate how the model performs on unseen data and analyze its predictions. Below are the steps performed:\n",
        "\n",
        "### 1. Define custom reviews\n",
        "- A list of reviews is created manually. These reviews include both positive and negative sentiments to check how well the model generalizes to new data.\n",
        "- Examples include:\n",
        "  - Highly positive reviews (e.g., *\\\"The movie was absolutely fantastic with a brilliant storyline.\"*).\n",
        "  - Strongly negative reviews (e.g., *\\\"I hated this film; it was a complete waste of time.\"*).\n",
        "  - Neutral or mixed reviews (e.g., *\\\"The plot was okay, but the characters were not very compelling.\"*).\n",
        "\n",
        "### 2. Preprocess reviews\n",
        "- The reviews are tokenized into word indices based on the IMDb vocabulary.\n",
        "- Words not found in the vocabulary are replaced with a special token.\n",
        "- The tokenized reviews are then padded to match the input length expected by the model (e.g., 200 tokens).\n",
        "\n",
        "### 3. Predict sentiment\n",
        "- The processed reviews are fed into the trained model for predictions.\n",
        "- The model outputs:\n",
        "  - A **confidence score**: How confident the model is in its prediction.\n",
        "  - A **sentiment label**: Either *Positive* or *Negative* based on the confidence score.\n",
        "\n",
        "### 4. Display results\n",
        "- Each review is displayed alongside:\n",
        "  - Its text content.\n",
        "  - The predicted sentiment (*Positive* or *Negative*).\n",
        "  - The confidence score (a value between 0 and 1).\n",
        "\n",
        "### Example output\n",
        "```text\n",
        "Review: The movie was absolutely fantastic with a brilliant storyline.\n",
        "Predicted Sentiment: Positive (Confidence: 0.95)\n",
        "--------------------------------------------------------------------------------\n",
        "Review: I hated this film; it was a complete waste of time.\n",
        "Predicted Sentiment: Negative (Confidence: 0.89)\n",
        "\n"
      ],
      "metadata": {
        "id": "xGNwYCvvsDWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess and predict sentiment for custom reviews\n",
        "def preprocess_and_predict(review, tokenizer, maxlen, model):\n",
        "    # Tokenize the input review\n",
        "    tokens = [tokenizer.get(word, 2) + 3 for word in review.split()]\n",
        "    tokens = pad_sequences([tokens], maxlen=maxlen, padding='post')\n",
        "\n",
        "    # Predict sentiment\n",
        "    prediction = model.predict(tokens)[0][0]\n",
        "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "\n",
        "    # Display results\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Sentiment: {sentiment} (Confidence: {prediction:.2f})\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    return sentiment, prediction"
      ],
      "metadata": {
        "id": "gLY90Z-KueIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example custom reviews for testing\n",
        "custom_revews = [\n",
        "    \"The movie was absolutely fantastic with a brilliant storyline.\",\n",
        "    \"I hated this film; it was a complete waste of time.\",\n",
        "    \"The plot was okay, but the characters were not very compelling.\",\n",
        "    \"What an amazing performance by the lead actor!\",\n",
        "    \"This movie was just boring. I fell asleep halfway through.\"\n",
        "]\n",
        "\n",
        "# Create a tokenizer mapping from IMDb dataset (like `index_to_word`)\n",
        "index_to_word = imdb.get_word_index()\n",
        "word_to_index = {key: value for key, value in index_to_word.items()}\n",
        "\n",
        "# Test custom reviews\n",
        "for review in custom_revews:\n",
        "    preprocess_and_predict(review, word_to_index, maxlen, model)"
      ],
      "metadata": {
        "id": "DT7Yrhmlveeb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}